{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import re\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlOriginalSentence='/home/farrukh/Work/Machine Translation/Datasets/en-ru/original'\n",
    "urlTargetSentence='/home/farrukh/Work/Machine Translation/Datasets/en-ru/target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(url,intInd,lastInd):\n",
    "    sentences=[]\n",
    "    with open(url,'r') as f:\n",
    "        sentences=f.read().split('\\n')[intInd:lastInd]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "startInd=0\n",
    "lastInd=25000\n",
    "originalSentence=loadDataset(urlOriginalSentence,startInd,lastInd)\n",
    "targetSentence=loadDataset(urlTargetSentence,startInd,lastInd)\n",
    "\n",
    "originalEvalSentence=loadDataset(urlOriginalSentence,lastInd,lastInd+1000)\n",
    "targetEvalSentence=loadDataset(urlTargetSentence,lastInd,lastInd+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalSentence=originalSentence[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSentence=targetSentence[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "max_length_original=50 \n",
    "max_length_target=50\n",
    "print(max_length_original)\n",
    "print(max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanOriginalSentence(sentences,wordList=None, addHeaders=False,max_length=None):\n",
    "    cleanedSentences=[]\n",
    "    for sentence in sentences:\n",
    "        sentence=re.sub('[.,\\'\\\"?~!#@$%^&*()]+',\"\",sentence.lower())\n",
    "        sentence=re.sub(\"[ ]+\",\" \",sentence)\n",
    "        sentence=re.sub('[^a-zA-Z]+',\" \",sentence)\n",
    "        if wordList is not None:\n",
    "            temp=[]\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word in wordList:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('unk')\n",
    "            if len(temp)>max_length-2:\n",
    "                temp=temp[:max_length-2]\n",
    "            sentence=\" \".join(temp)\n",
    "        sentence=' '.join([w for w in sentence.split(' ') if len(w)>1 ])\n",
    "        if addHeaders==True:\n",
    "            sentence='<start> ' + sentence+ ' <end>'\n",
    "\n",
    "        cleanedSentences.append(sentence)\n",
    "    return cleanedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTargetSentence(sentences,wordList=None,addHeaders=False,max_length=None):\n",
    "    cleanedSentences=[]\n",
    "    for sentence in sentences:\n",
    "        sentence=re.sub('[.,\\'\\\"?~!#@$%^&*()]+',\"\",sentence.lower())\n",
    "        sentence=re.sub(\"[ ]+\",\" \",sentence)\n",
    "        sentence=re.sub('[^а-яА-Я]+',\" \",sentence) \n",
    "        \n",
    "        if wordList is not None:\n",
    "            temp=[]\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word in wordList:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('unk')\n",
    "            if len(temp)>max_length-2:\n",
    "                temp=temp[:max_length-2]\n",
    "            sentence=\" \".join(temp)\n",
    "            \n",
    "        sentence=' '.join([w for w in sentence.split(\" \")])\n",
    "        if addHeaders==True:\n",
    "            sentence='<start> ' + sentence+ ' <end>'\n",
    "\n",
    "        cleanedSentences.append(sentence)\n",
    "    return cleanedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580\n",
      "16512\n"
     ]
    }
   ],
   "source": [
    "originalSentence=cleanOriginalSentence(originalSentence)\n",
    "counterOriginal=Counter(\" \".join(originalSentence).split(\" \"))\n",
    "counterOriginal={word:k for word,k in counterOriginal.items() if k>2}\n",
    "print(len(counterOriginal))\n",
    "\n",
    "targetSentence=cleanTargetSentence(targetSentence)\n",
    "counterTarget=Counter(\" \".join(targetSentence).split(\" \"))\n",
    "counterTarget={word:k for word,k in counterTarget.items() if k>1}\n",
    "print(len(counterTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalSentence=cleanOriginalSentence(originalSentence,counterOriginal.keys(),True,max_length_original)\n",
    "originalEvalSentence=cleanOriginalSentence(originalEvalSentence,counterOriginal.keys(),True,max_length_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSentence=cleanTargetSentence(targetSentence,counterTarget.keys(),True,max_length_target)\n",
    "targetEvalSentence=cleanTargetSentence(targetEvalSentence,counterTarget.keys(),True,max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences,max_length):\n",
    "    tokenizer=keras.preprocessing.text.Tokenizer(filters=\"\",)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tensor=tokenizer.texts_to_sequences(sentences)\n",
    "    tensor=keras.preprocessing.sequence.pad_sequences(tensor,padding='post',maxlen=max_length)\n",
    "    return tensor,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original,originalTokenizer=tokenize(originalSentence,max_length_original)\n",
    "target,targetTokenizer=tokenize(targetSentence,max_length_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "originalEval=keras.preprocessing.sequence.pad_sequences(originalTokenizer.texts_to_sequences(originalEvalSentence),maxlen=max_length_original,padding='post')\n",
    "\n",
    "targetEval=keras.preprocessing.sequence.pad_sequences(targetTokenizer.texts_to_sequences(targetEvalSentence),maxlen=max_length_target,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=24\n",
    "steps_per_epoch=len(original)//batch_size\n",
    "embedding_dims=300\n",
    "units=300\n",
    "vocab_original_size=len(originalTokenizer.index_word)+1\n",
    "vocab_target_size=len(targetTokenizer.index_word)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6583"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_original_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16487"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(encoderInp,decoderInp,max_length,batch_size,vocab_size):\n",
    "    X1,X2,y=[],[],[]\n",
    "    n=0\n",
    "    while 1:\n",
    "        for enc,dec in zip(encoderInp,decoderInp):\n",
    "            n+=1\n",
    "            for i in range(1,len(dec)):\n",
    "                in_seq,out_seq=dec[:i],dec[i]\n",
    "                in_seq=keras.preprocessing.sequence.pad_sequences([in_seq],maxlen=max_length)[0]\n",
    "                out_seq=keras.utils.to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                X1.append(enc[::-1])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "            if n==batch_size:\n",
    "                n=0\n",
    "                yield ([np.array(X1),np.array(X2)],np.array(y))\n",
    "                X1,X2,y=[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=generator(original,target,max_length_target,batch_size,vocab_target_size)\n",
    "val_generator=generator(originalEval,targetEval,max_length_target,batch_size,vocab_target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0304 13:46:10.360821 140243442886400 deprecation.py:506] From /home/farrukh/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0304 13:46:10.363305 140243442886400 deprecation.py:506] From /home/farrukh/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "inputEncoder=keras.Input((max_length_original,),name='encoderInput')\n",
    "\n",
    "embedEnc=keras.layers.Embedding(vocab_original_size,embedding_dims,name='Encoder_Embedding')\n",
    "gruEnc=keras.layers.GRU(units,return_sequences=True,return_state=True,dropout=0.25,recurrent_dropout=0.25,name='Encoder_GRU1')\n",
    "se1=embedEnc(inputEncoder)\n",
    "se2_out,se2_hidden=gruEnc(se1) \n",
    "'''\n",
    "se2_out shape= (batch_size,max_length,units)\n",
    "se2_hidden shape= (batch_size,units)\n",
    "'''\n",
    "\n",
    "inputDecoder=keras.Input((max_length_target,),name='decoderInput')\n",
    "embedDec=keras.layers.Embedding(vocab_target_size,embedding_dims,name='Decoder_Embedding')\n",
    "gruDec=keras.layers.GRU(units,dropout=0.25,recurrent_dropout=0.25,return_sequences=True,return_state=True,name='Decoder_GRU1') ## try with statefull\n",
    "denseDec=keras.layers.Dense(vocab_target_size,activation='softmax')\n",
    "sd1=embedDec(inputDecoder)\n",
    "sd2_out,sd2_hidden,=gruDec(sd1,initial_state=[se2_hidden])\n",
    "sd3=denseDec(sd2_out[:,-1])\n",
    "model=keras.Model([inputEncoder,inputDecoder],sd3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_model=keras.Model(inputEncoder,[se2_hidden])\n",
    "\n",
    "decoder_state_inp=keras.Input((units,))\n",
    "decoder_out,decoder_state_out,=gruDec(sd1,initial_state=[decoder_state_inp])\n",
    "eval_out=denseDec(decoder_out[:,-1])\n",
    "decoder_model=keras.Model([decoder_state_inp,inputDecoder],[eval_out,decoder_state_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/farrukh/Work/Image Caption/Glove/glove.6B.300d.txt') as f:\n",
    "    lines=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index={}\n",
    "for line in lines.split('\\n'):\n",
    "    values=line.split(\" \")\n",
    "    word=values[0]\n",
    "    values=np.asarray(values[1:],dtype='float64')\n",
    "    if word in originalTokenizer.word_index.keys():\n",
    "        embedding_index[word]=values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((vocab_original_size,embedding_dims),dtype='float64')\n",
    "\n",
    "for word,i in originalTokenizer.word_index.items():\n",
    "    vec=embedding_index.get(word,None)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i]=vec\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable=False\n",
    "del(embedding_index)\n",
    "del(embedding_matrix)\n",
    "del(lines)\n",
    "del(counterOriginal)\n",
    "del(counterTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0304 13:48:42.713305 140243442886400 training.py:2197] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoderInput (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoderInput (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_Embedding (Embedding)   (None, 50, 300)      1974900     encoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Embedding (Embedding)   (None, 50, 300)      4946100     decoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_GRU1 (GRU)              [(None, 50, 300), (N 540900      Encoder_Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_GRU1 (GRU)              [(None, 50, 300), (N 540900      Decoder_Embedding[0][0]          \n",
      "                                                                 Encoder_GRU1[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 300)]        0           Decoder_GRU1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16487)        4962587     tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 14,940,287\n",
      "Trainable params: 12,965,387\n",
      "Non-trainable params: 1,974,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=keras.callbacks.ModelCheckpoint('/home/farrukh/Work/Machine Translation/NMT_simple.hdf5',save_weights_only=True,monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('/home/farrukh/Work/Machine Translation/NMT_simple2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator,steps_per_epoch=len(target)//batch_size,epochs=10,callbacks=[callback],verbose=True,validation_data=val_generator,\n",
    "                    validation_steps=len(targetEval),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0224 02:30:42.032688 139999204845312 training.py:2197] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0224 02:30:42.269499 139999204845312 deprecation.py:323] From /home/farrukh/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041/1041 [==============================] - 716s 687ms/step - loss: 9.1255 - acc: 0.6680 - val_loss: 4.1282 - val_acc: 0.5669\n",
      "Epoch 2/10\n",
      "1041/1041 [==============================] - 710s 682ms/step - loss: 3.2613 - acc: 0.6599 - val_loss: 3.5266 - val_acc: 0.5725\n",
      "Epoch 3/10\n",
      "1041/1041 [==============================] - 710s 682ms/step - loss: 3.0276 - acc: 0.6656 - val_loss: 3.4082 - val_acc: 0.5789\n",
      "Epoch 4/10\n",
      "1041/1041 [==============================] - 710s 683ms/step - loss: 2.7706 - acc: 0.6727 - val_loss: 3.2377 - val_acc: 0.5970\n",
      "Epoch 5/10\n",
      "1041/1041 [==============================] - 716s 688ms/step - loss: 2.6338 - acc: 0.6892 - val_loss: 3.0907 - val_acc: 0.6186\n",
      "Epoch 6/10\n",
      "1041/1041 [==============================] - 711s 683ms/step - loss: 2.3397 - acc: 0.7139 - val_loss: 2.8721 - val_acc: 0.6426\n",
      "Epoch 7/10\n",
      " 784/1041 [=====================>........] - ETA: 2:03 - loss: 2.2199 - acc: 0.7196"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3b81c95e2f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalEval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetEval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_target_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit_generator(train_generator,steps_per_epoch=len(target)//batch_size,epochs=10,callbacks=[callback],verbose=True,validation_data=val_generator,\n\u001b[0;32m----> 5\u001b[0;31m                     validation_steps=len(targetEval),shuffle=False)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=24\n",
    "train_generator=generator(original,target,max_length_target,batch_size,vocab_target_size)\n",
    "val_generator=generator(originalEval,targetEval,max_length_target,batch_size,vocab_target_size)\n",
    "model.fit_generator(train_generator,steps_per_epoch=len(target)//batch_size,epochs=10,callbacks=[callback],verbose=True,validation_data=val_generator,\n",
    "                    validation_steps=len(targetEval),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTokenizer.index_word[0]='unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " кроме того правительство австралии весьма unk и что правительство квазулу состоит из сторон сама по себе и пересмотр членского состава совета безопасности <end>\n"
     ]
    }
   ],
   "source": [
    "inp1=(originalEval[3])[::-1].reshape(1,-1)\n",
    "inp2='<start>'\n",
    "pred=[targetTokenizer.word_index[inp2]]\n",
    "text=\"\"\n",
    "inp1=encoder_model.predict([inp1])\n",
    "for i in range(max_length_target):\n",
    "    seq=[w for w in pred]\n",
    "    seq=keras.preprocessing.sequence.pad_sequences([seq],maxlen=max_length_target)\n",
    "    y_hat,inp1=decoder_model.predict([inp1,seq])\n",
    "    y_hat=np.argmax(y_hat)\n",
    "    pred.append(y_hat)\n",
    "    text=text+ \" \" + targetTokenizer.index_word[y_hat]\n",
    "    if targetTokenizer.index_word[y_hat]=='<end>':\n",
    "        break\n",
    "    decoder_model.reset_states()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start>  кроме того правительство unk предлагает чтобы новые члены совета unk региональными группами и при этом проводилась ротация <end>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetEvalSentence[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> in addition the government of madagascar proposes that the new members of the council should be elected by regional group and by rotation <end>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalEvalSentence[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targetEval[321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
